#+TITLE: SL07 Hashtables

* Hash tables
** Dictionaries

- Dictionaries store elemets so that they can be located quickly using keys
- If order (methods such as min, max. succsessor, predecessor) is not required it is enough to check for equality.
- Operations that require ordering are still possible (e.g., scanning the entire database) but can not use the dictionary access structure
  - Usually all elements must be compared, which is slow
  - Can be OK if it is rare enough
*** Stats
- BST: insert, search, and delete in =O(h)= time
- RBT: insert, search, and delete in =O(lg n)= time
- Hashing: insert, search, and delete in =O(1)= time
  - Other dictionary operations (min, succ, ...) are not supported
  - Hash table is expensive and difficult to grow and shrink.
  - Application: symbol table for identifiers in a compiler, customer IDs, ...

*** Hash tables vs arrays
- Hash tables are a generalization of arrays
- Big advantage of arrays consists in direct addressing
  - Ability to access arbitrary array position in =O(1)= time

Diffrent data structures to realize dictionaries
- Array
- Linked lists
- Hash tables
- BST
- RBT

** Hash tables
- We use a hash table of size m to avoid wasting space.
- A hash table is like an array, but with a function to map the large range into one that we can manage efficiently.
- For instance take the original key, modulo the size of the relatively small hash table, and use that as an index.
- Example: insert (9635-8904, Jens) into a hash table with five slots (m = 5):
  - 96358904 mod 5 = 4

 The Hash table data structure stores elements in key-value pairs where

- *Key*- unique integer that is used for indexing the values
- *Value* - data that are associated with keys.
  
*** Using a Hashfunction
In a hash table, a new index is processed using the keys. And, the element corresponding to that key is stored in the index. This process is called *hashing*.

Let =k= be a key and =h(x)= be a hash function.
Here, =h(k)= will give us a new index to store the element linked with =k=.

**** Hashing
Hashing is a technique of mapping a large set of arbitrary data to tabular indexes using a hash function. It is a method for representing dictionaries for large datasets.

It allows lookups, updating and retrieval operation to occur in a constant time i.e. =O(1)=.

***** Why hashing is needed?
After storing a large amount of data, we need to perform various operations on these data. Lookups are inevitable for the datasets. Linear search and binary search perform lookups/search with time complexity of =O(n)= and =O(log n)= respectively. As the size of the dataset increases, these complexities also become significantly high which is not acceptable.

We need a technique that does not depend on the size of data. Hashing allows lookups to occur in constant time i.e. =O(1)=.
***** Hash function
A hash function is used for mapping each element of a dataset to indexes in the table.

For more information on hash table, collision resolution techniques and hash functions, please visit Hash Table.

* Colision Resolution

** Chaining
- The simplest approach is chaining.
- Chaining maintains a table of links, indexed by the keys, to lists of items with the same key.
- *Pros*: easy, flexible, hash table never gets full
- *Cons*: stores elements outside hash table, additional space

** Open Addressing
- The idea is to store all items in the hash table itself
  - Each table entry contains either an item or is empty
  - No lists and no items are stored outside the table
- A drawback is that the table can be filled up: load factor =a= never exceeds 1
- Advantage of open adressing: avoid pointers
  - No need to follow pointers when accessing items
  - Simply compute sequence of slots to be examined
  - Larger number of slots for same amount of memory

Open addressing uses an extended hash function that includes the probe numbers =h: U x {1, 2, . . . , m} \to {1, 2, . . . , m}=
for all =k: <h(k,1),...,h(k,m)>= is permutation of =<1,...,m>=

- All dictionary operations systematically probe table slots
- Sequence of positions probed depends on given key

*** Linear Probing
- Uses a hash function of the from
  - =h(k,i) = (h'(k) + ci) mod m=
  - where =h'= is a hash function
- We try out (probe) a number of positions:
  - =NextLoc = (PreviousLoc + StepSize) mod m=
- The step size should be chosen such that all locations are probed.
- Often a step size of one =(c=1)= is chosen. This guarantees that all locations are probed. (but data not distributed well)

*Steps*
- If the current location is used, try the next table location:
- Lookups walk along the table until the key or an empty slot is found
- Uses less memory than chaining: one does not have to store the links
- Slower than chaining: one might have to probe the table for a long time

#+begin_src c
//Algo: LinearProbingInsert(k)
if table is full then return error;
probe = h(k);
while HT[probe] is used do
    probe = (probe+1) mod m
table[probe] = k;
#+end_src

*** Double Hashing
Two hash functions:
1. =h1= determines the initial location
2. =h2= determines the step size
3. =h(k,i) = (h1(k) + i*h2(k) mod m=

- Initial probe is to position =HT[h1(k)]=, successive probe positions are offset by =h2(k)=.

#+begin_src c
//Search:
//Algo: DoubleHashingInsert(HT,k)
i = 0;
repeat
    probe = (h1(k) + i*h2(k)) mod m;
    i = i+1;
until (HT[probe] is free ‚ i>m);
if i>m then Error: hash table overflow;
else HT[probe] = k;
#+end_src

- =h2(k)= must be relatively prime to hash table size =m=
  - =m= a power of =2=, =h2= always produces an odd number
  - =m= prime, =h2= always produces a positive integer < =m=
- Example: =m= prime and =h2(k)= positive integer < =m=
  - Let =h1(k)=(kmodm)+1=, =h2(k)=(k mod m')+1=
  - Choose =m'= to be slightly less than =m= (say =m' = m-1=)
  - Given =k=123456= , =m=701= , =m' =700: h1(k)=81= , =h2(k) = 257= (first probe to 81, then every 257th slot)
- Cost of searching in open-address hash table with =a = n/m < 1=
  - unsuccessful: =1/(1-a)= 
  - successful: =ln(1/(1 - a))/a=

*Dictionary operations*
- Inserting and searching are relatively straightforward
  - At least if one assumes that there are no deletions
- Deletion is difficult: cannot mark deleted slot as empty
  - Solution: use special value to mark a deleted slot
  - This affects all hash table functions (e.g., the simple procedures for insertion must be extended)

 
** Hashing with Open Addressing
Assume hashing with open addressing as a collision resolution strategy.
1. Define the hash table and give a procedure ti initialize it.
2. Give a procedure to insert, search and delete an element.
   
#+begin_src c
  struct elem {
    int key;
    int status;
    // 0: OCCupied, -1: EMPty, -2: DELeted
  };
  struct elem HT [7000];
#+end_src

#+begin_src c
  //Algo: HTinit(HT)
  for i = 0 to m-1 do HT[i].status = EMP;
#+end_src

*Hash table insert*
#+begin_src c
  //Algo: HTinsert(HT, k)
  i = -1;
  repeat
      i++; probe = h(k,i);

  until i>=m || HT[probe].status != OCC;

  if i>=m then return -1;

  HT[probe].status = 0;
  HT[probe].key = k;
  return probe
#+end_src

*Hash table search*
#+begin_src c
//Algo: HTsearch(HT,k)
i = -1;
repeat
    i++; probe = h(k,i);
until i >= m /*searched unsuccseful*/|| /*Elem found*/ (HT[probe].status==OCC && HT[probe].key==k) || HT[probe].status==EMP (/*end of chain*/);
if i>=m || HT[probe].status==EMP then return -1;
return probe
#+end_src

*Hash table delete*
#+begin_src c
//Algo: HTdelete(HT,k)
i = -1;
repeat
    i++; probe = h(k,i);
until i>=m /*searched m slots without finding k*/ ‚ HT[probe].status==EMP /*end of chain*/|| HT[probe].status==OCC && HT[probe].key==k /*element found*/;

if i>=m || HT[probe].status==EMP then return -1;
HT[probe].status = DEL;
return probe
#+end_src

